{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Distribution Analysis: Single-Pooled IVF+SQ8\n",
    "\n",
    "Analyzing pairwise cosine similarity distributions for the **single-pooled** strategy with **IVF+SQ8** index.\n",
    "\n",
    "Three analysis approaches:\n",
    "1. Random pair sampling\n",
    "2. Anchor-based sampling\n",
    "3. Nearest-neighbor distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Configuration\n",
    "STRATEGY = \"single-pooled\"\n",
    "INDEX_TYPE = \"ivf_sq8\"\n",
    "INDEX_PATH = Path(\"../data/indices/full_synonyms/minilm_single-pooled_ivf_sq8.faiss\")\n",
    "METADATA_PATH = Path(\"../data/indices/full_synonyms/minilm_single-pooled_ivf_sq8.metadata.json\")\n",
    "\n",
    "# Check GPU availability\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "print(f\"GPU available: {USE_GPU}\")\n",
    "if USE_GPU:\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Index and Extract Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS index\n",
    "print(f\"Loading index from {INDEX_PATH}...\")\n",
    "index = faiss.read_index(str(INDEX_PATH))\n",
    "n_vectors = index.ntotal\n",
    "dim = index.d\n",
    "print(f\"Index loaded: {n_vectors:,} vectors, dimension {dim}\")\n",
    "\n",
    "# Load metadata\n",
    "with open(METADATA_PATH, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "print(f\"Metadata loaded: {len(metadata):,} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vectors from the index\n",
    "# For HNSW+SQ8, we need to reconstruct vectors\n",
    "print(\"Reconstructing vectors from index...\")\n",
    "\n",
    "# Reconstruct all vectors (for random pair sampling)\n",
    "# This may take a moment for large indices\n",
    "vectors = np.zeros((n_vectors, dim), dtype=np.float32)\n",
    "for i in range(n_vectors):\n",
    "    vectors[i] = index.reconstruct(i)\n",
    "\n",
    "# Normalize vectors (should already be normalized, but ensure)\n",
    "norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "vectors = vectors / np.clip(norms, 1e-8, None)\n",
    "\n",
    "print(f\"Vectors reconstructed: shape {vectors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Pair Sampling\n",
    "\n",
    "Sample 100K random pairs of vectors and compute their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PAIRS = 100_000\n",
    "\n",
    "# Sample random pairs\n",
    "np.random.seed(42)\n",
    "idx1 = np.random.randint(0, n_vectors, size=N_PAIRS)\n",
    "idx2 = np.random.randint(0, n_vectors, size=N_PAIRS)\n",
    "\n",
    "# Ensure no self-pairs\n",
    "same_mask = idx1 == idx2\n",
    "idx2[same_mask] = (idx2[same_mask] + 1) % n_vectors\n",
    "\n",
    "print(f\"Sampled {N_PAIRS:,} random pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarities using GPU if available\n",
    "if USE_GPU:\n",
    "    print(\"Computing similarities on GPU...\")\n",
    "    v1 = torch.tensor(vectors[idx1], device='cuda')\n",
    "    v2 = torch.tensor(vectors[idx2], device='cuda')\n",
    "    random_pair_similarities = (v1 * v2).sum(dim=1).cpu().numpy()\n",
    "    del v1, v2\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Computing similarities on CPU...\")\n",
    "    random_pair_similarities = np.sum(vectors[idx1] * vectors[idx2], axis=1)\n",
    "\n",
    "print(f\"Computed {len(random_pair_similarities):,} similarities\")\n",
    "print(f\"Mean: {random_pair_similarities.mean():.4f}\")\n",
    "print(f\"Std: {random_pair_similarities.std():.4f}\")\n",
    "print(f\"Min: {random_pair_similarities.min():.4f}\")\n",
    "print(f\"Max: {random_pair_similarities.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(random_pair_similarities, bins=100, ax=ax, stat='density')\n",
    "ax.axvline(random_pair_similarities.mean(), color='red', linestyle='--', label=f'Mean: {random_pair_similarities.mean():.3f}')\n",
    "ax.axvline(np.median(random_pair_similarities), color='orange', linestyle='--', label=f'Median: {np.median(random_pair_similarities):.3f}')\n",
    "ax.set_xlabel('Cosine Similarity')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title(f'Random Pair Similarity Distribution\\n{STRATEGY} / {INDEX_TYPE} ({N_PAIRS:,} pairs)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Percentiles\n",
    "percentiles = [1, 5, 25, 50, 75, 95, 99]\n",
    "print(\"\\nPercentiles:\")\n",
    "for p in percentiles:\n",
    "    print(f\"  {p}th: {np.percentile(random_pair_similarities, p):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Anchor-Based Sampling\n",
    "\n",
    "Select 1000 anchor vectors, compute similarity to 100 random others for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ANCHORS = 1000\n",
    "N_OTHERS = 100\n",
    "\n",
    "np.random.seed(43)\n",
    "anchor_indices = np.random.choice(n_vectors, size=N_ANCHORS, replace=False)\n",
    "anchor_similarities = []\n",
    "\n",
    "print(f\"Computing anchor-based similarities ({N_ANCHORS} anchors x {N_OTHERS} others)...\")\n",
    "\n",
    "if USE_GPU:\n",
    "    anchors_gpu = torch.tensor(vectors[anchor_indices], device='cuda')\n",
    "    for i, anchor_idx in enumerate(anchor_indices):\n",
    "        # Sample N_OTHERS random indices (excluding anchor)\n",
    "        others = np.random.choice(np.delete(np.arange(n_vectors), anchor_idx), size=N_OTHERS, replace=False)\n",
    "        others_gpu = torch.tensor(vectors[others], device='cuda')\n",
    "        sims = (anchors_gpu[i:i+1] * others_gpu).sum(dim=1).cpu().numpy()\n",
    "        anchor_similarities.extend(sims)\n",
    "    del anchors_gpu, others_gpu\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    for anchor_idx in anchor_indices:\n",
    "        others = np.random.choice(np.delete(np.arange(n_vectors), anchor_idx), size=N_OTHERS, replace=False)\n",
    "        sims = np.dot(vectors[others], vectors[anchor_idx])\n",
    "        anchor_similarities.extend(sims)\n",
    "\n",
    "anchor_similarities = np.array(anchor_similarities)\n",
    "print(f\"Computed {len(anchor_similarities):,} similarities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.histplot(anchor_similarities, bins=100, ax=ax, stat='density')\n",
    "ax.axvline(anchor_similarities.mean(), color='red', linestyle='--', label=f'Mean: {anchor_similarities.mean():.3f}')\n",
    "ax.axvline(np.median(anchor_similarities), color='orange', linestyle='--', label=f'Median: {np.median(anchor_similarities):.3f}')\n",
    "ax.set_xlabel('Cosine Similarity')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title(f'Anchor-Based Similarity Distribution\\n{STRATEGY} / {INDEX_TYPE} ({N_ANCHORS} anchors x {N_OTHERS} others)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStats:\")\n",
    "print(f\"Mean: {anchor_similarities.mean():.4f}\")\n",
    "print(f\"Std: {anchor_similarities.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Nearest-Neighbor Distribution\n",
    "\n",
    "Use FAISS to find top-K neighbors for random query vectors. This shows how \"tight\" clusters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_QUERIES = 1000\n",
    "K_NEIGHBORS = 50\n",
    "\n",
    "np.random.seed(44)\n",
    "query_indices = np.random.choice(n_vectors, size=N_QUERIES, replace=False)\n",
    "query_vectors = vectors[query_indices]\n",
    "\n",
    "print(f\"Searching for {K_NEIGHBORS} nearest neighbors for {N_QUERIES} queries...\")\n",
    "\n",
    "# Search for neighbors\n",
    "distances, indices = index.search(query_vectors, K_NEIGHBORS + 1)  # +1 to exclude self\n",
    "\n",
    "# Remove self-matches (first result is usually the query itself)\n",
    "raw_distances = distances[:, 1:].flatten()  # Skip first column (self)\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORTANT FIX: Converting FAISS L2\u00b2 distances to cosine similarity\n",
    "# =============================================================================\n",
    "# FAISS HNSW+SQ8 (and IVF+SQ8) indices use L2 metric by default and return\n",
    "# SQUARED L2 distances (L2\u00b2), not cosine similarities or regular L2.\n",
    "#\n",
    "# For normalized vectors (unit length), the relationship is:\n",
    "#   L2\u00b2 = ||a - b||\u00b2 = ||a||\u00b2 + ||b||\u00b2 - 2(a\u00b7b) = 1 + 1 - 2*cos(\u03b8) = 2(1 - cos(\u03b8))\n",
    "#\n",
    "# Therefore, to convert L2\u00b2 to cosine similarity:\n",
    "#   cos(\u03b8) = 1 - L2\u00b2/2\n",
    "#\n",
    "# BUG THAT WAS FIXED: Originally we incorrectly squared the distances again,\n",
    "# using `1 - (raw_distances ** 2) / 2`, which produced invalid values > 1.\n",
    "# The correct formula is simply `1 - raw_distances / 2` since FAISS already\n",
    "# returns squared distances.\n",
    "# =============================================================================\n",
    "nn_similarities = 1 - raw_distances / 2\n",
    "\n",
    "# Clip to [-1, 1] range (quantization noise can cause slight overflow)\n",
    "nn_similarities = np.clip(nn_similarities, -1.0, 1.0)\n",
    "\n",
    "print(f\"Found {len(nn_similarities):,} neighbor similarities\")\n",
    "print(f\"Raw L2\u00b2 distances - Min: {raw_distances.min():.4f}, Max: {raw_distances.max():.4f}\")\n",
    "print(f\"Converted cosine similarities - Min: {nn_similarities.min():.4f}, Max: {nn_similarities.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram with special attention to score=1.0 (identical vectors)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Full distribution\n",
    "ax = axes[0]\n",
    "sns.histplot(nn_similarities, bins=100, ax=ax, stat='density')\n",
    "ax.axvline(nn_similarities.mean(), color='red', linestyle='--', label=f'Mean: {nn_similarities.mean():.3f}')\n",
    "ax.set_xlabel('Cosine Similarity (Inner Product)')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title(f'Nearest Neighbor Similarity Distribution\\n{STRATEGY} / {INDEX_TYPE}')\n",
    "ax.legend()\n",
    "\n",
    "# Zoom in on high similarity region (0.9 - 1.0)\n",
    "ax = axes[1]\n",
    "high_sim = nn_similarities[nn_similarities >= 0.9]\n",
    "sns.histplot(high_sim, bins=50, ax=ax, stat='count')\n",
    "ax.axvline(0.99, color='red', linestyle='--', alpha=0.5)\n",
    "ax.axvline(1.0, color='darkred', linestyle='-', label='Score = 1.0')\n",
    "ax.set_xlabel('Cosine Similarity')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(f'High Similarity Region (>= 0.9)\\n{len(high_sim):,} / {len(nn_similarities):,} = {100*len(high_sim)/len(nn_similarities):.1f}%')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze identical/near-identical vectors\n",
    "identical_count = np.sum(nn_similarities >= 0.9999)\n",
    "near_identical_count = np.sum(nn_similarities >= 0.99)\n",
    "high_sim_count = np.sum(nn_similarities >= 0.9)\n",
    "\n",
    "print(\"Nearest Neighbor Analysis:\")\n",
    "print(f\"  Total neighbor pairs: {len(nn_similarities):,}\")\n",
    "print(f\"  Identical (>= 0.9999): {identical_count:,} ({100*identical_count/len(nn_similarities):.2f}%)\")\n",
    "print(f\"  Near-identical (>= 0.99): {near_identical_count:,} ({100*near_identical_count/len(nn_similarities):.2f}%)\")\n",
    "print(f\"  High similarity (>= 0.9): {high_sim_count:,} ({100*high_sim_count/len(nn_similarities):.2f}%)\")\n",
    "print(f\"\\nDistribution stats:\")\n",
    "print(f\"  Mean: {nn_similarities.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(nn_similarities):.4f}\")\n",
    "print(f\"  Std: {nn_similarities.std():.4f}\")\n",
    "print(f\"  Min: {nn_similarities.min():.4f}\")\n",
    "print(f\"  Max: {nn_similarities.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Explore how K affects the NN similarity distribution\n",
    "# =============================================================================\n",
    "# Question from Andrew: does the peak shift towards 1 with smaller K?\n",
    "# \n",
    "# Expected behavior: With smaller K, we only look at the closest neighbors,\n",
    "# so the mean similarity should be higher (peak closer to 1.0).\n",
    "# With larger K, we include more distant neighbors, lowering the mean.\n",
    "#\n",
    "# Same L2\u00b2 -> cosine conversion as above: cos(\u03b8) = 1 - L2\u00b2/2\n",
    "# =============================================================================\n",
    "\n",
    "K_values = [5, 10, 25, 50, 100]\n",
    "fig, axes = plt.subplots(1, len(K_values), figsize=(20, 4))\n",
    "\n",
    "for ax, k in zip(axes, K_values):\n",
    "    # Search with this K\n",
    "    dists, _ = index.search(query_vectors, k + 1)\n",
    "    raw_dists = dists[:, 1:].flatten()\n",
    "    # Convert L2\u00b2 to cosine similarity\n",
    "    sims = np.clip(1 - raw_dists / 2, -1.0, 1.0)\n",
    "    \n",
    "    sns.histplot(sims, bins=50, ax=ax, stat='density')\n",
    "    ax.axvline(sims.mean(), color='red', linestyle='--', alpha=0.7)\n",
    "    ax.set_xlabel('Cosine Similarity')\n",
    "    ax.set_title(f'K={k}\\nMean: {sims.mean():.3f}')\n",
    "    ax.set_xlim(-0.2, 1.1)\n",
    "\n",
    "plt.suptitle('NN Similarity Distribution vs K', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary stats for each K\n",
    "print(\"\\nSummary by K:\")\n",
    "print(f\"{'K':>5} | {'Mean':>8} | {'Median':>8} | {'% >= 0.9':>10} | {'% >= 0.99':>10}\")\n",
    "print(\"-\" * 55)\n",
    "for k in K_values:\n",
    "    dists, _ = index.search(query_vectors, k + 1)\n",
    "    raw_dists = dists[:, 1:].flatten()\n",
    "    sims = np.clip(1 - raw_dists / 2, -1.0, 1.0)\n",
    "    pct_90 = 100 * np.sum(sims >= 0.9) / len(sims)\n",
    "    pct_99 = 100 * np.sum(sims >= 0.99) / len(sims)\n",
    "    print(f\"{k:>5} | {sims.mean():>8.4f} | {np.median(sims):>8.4f} | {pct_90:>9.2f}% | {pct_99:>9.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three distributions\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "sns.kdeplot(random_pair_similarities, ax=ax, label=f'Random Pairs (n={len(random_pair_similarities):,})', linewidth=2)\n",
    "sns.kdeplot(anchor_similarities, ax=ax, label=f'Anchor-Based (n={len(anchor_similarities):,})', linewidth=2)\n",
    "sns.kdeplot(nn_similarities, ax=ax, label=f'Nearest Neighbors (n={len(nn_similarities):,})', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Cosine Similarity')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title(f'Similarity Distribution Comparison\\n{STRATEGY} / {INDEX_TYPE}')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "import pandas as pd\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'Metric': ['Mean', 'Median', 'Std', 'Min', 'Max', '% >= 0.9', '% >= 0.99'],\n",
    "    'Random Pairs': [\n",
    "        f\"{random_pair_similarities.mean():.4f}\",\n",
    "        f\"{np.median(random_pair_similarities):.4f}\",\n",
    "        f\"{random_pair_similarities.std():.4f}\",\n",
    "        f\"{random_pair_similarities.min():.4f}\",\n",
    "        f\"{random_pair_similarities.max():.4f}\",\n",
    "        f\"{100*np.sum(random_pair_similarities >= 0.9)/len(random_pair_similarities):.2f}%\",\n",
    "        f\"{100*np.sum(random_pair_similarities >= 0.99)/len(random_pair_similarities):.2f}%\"\n",
    "    ],\n",
    "    'Anchor-Based': [\n",
    "        f\"{anchor_similarities.mean():.4f}\",\n",
    "        f\"{np.median(anchor_similarities):.4f}\",\n",
    "        f\"{anchor_similarities.std():.4f}\",\n",
    "        f\"{anchor_similarities.min():.4f}\",\n",
    "        f\"{anchor_similarities.max():.4f}\",\n",
    "        f\"{100*np.sum(anchor_similarities >= 0.9)/len(anchor_similarities):.2f}%\",\n",
    "        f\"{100*np.sum(anchor_similarities >= 0.99)/len(anchor_similarities):.2f}%\"\n",
    "    ],\n",
    "    'Nearest Neighbors': [\n",
    "        f\"{nn_similarities.mean():.4f}\",\n",
    "        f\"{np.median(nn_similarities):.4f}\",\n",
    "        f\"{nn_similarities.std():.4f}\",\n",
    "        f\"{nn_similarities.min():.4f}\",\n",
    "        f\"{nn_similarities.max():.4f}\",\n",
    "        f\"{100*np.sum(nn_similarities >= 0.9)/len(nn_similarities):.2f}%\",\n",
    "        f\"{100*np.sum(nn_similarities >= 0.99)/len(nn_similarities):.2f}%\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SUMMARY: {STRATEGY} / {INDEX_TYPE}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(summary.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}